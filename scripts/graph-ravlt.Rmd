---
title: "ica_300: Graph Metric & RAVLT"
output: pdf_document
---


```{r, packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(rstatix)    
#library(ggpubr)     
#library(car)
library(MASS)
```

### Load data
```{r}
data <- read.csv("../data/flat_nihtb-graph_metrics.csv")
```

# RAVLT
The RAVLT is a neuropsychological measure of working memory. A list of 15 words is presented across 5 trials. The list is read aloud and participants are asked to recall as many words as they heard. After trials 1-5, a new list of 15 words (List B) is presented, and then the participant is asked to recall those words. On Trial 6, the participant is asked to recall as many words from the first list. 30 Minutes after the completion of recall on List B, the participant is asked to again recall the words from List A.

## RAVLT Immediate Learning
Immediate learning is scored as the sum from the scores of trials 1-5.
```{r, message=FALSE, warning=FALSE}
colors = c("#440154FF","#1565c0")
ggplot(data, aes(x = ravlt_imm, color = sex, fill = sex)) +
  geom_histogram(alpha = 0.7) +
  scale_color_manual(values = colors) +
  scale_fill_manual(values = colors) +
  facet_grid(col = vars(sex))
```

### Mean and Variance of Immediate Learning
Mean does not equal variance which violates the assumption of poisson regression. Modeling with poisson also results in Variance larger than the mean (overdispersion). Overdispersion is also indicated when residual deviance is substantially larger than the degrees of freedom. Models rely on negative binomial GLMs. In negative binomial regression, the coefficients are interpreted as for every unit increase in the independent variable, the expected log count of the dependent variable will change by the value of the coefficient. For binary variables, the coefficients are interpreted in reference to the baseline variable. These can be interpreted as the expected log counts for variable A is X units higher/lower than in the expected log counts for the reference group.  The constant tells us what is the expected log counts if all predictors are equal to zero. 
```{r}
data %>%
  summarise(count = n(),
            mean = mean(ravlt_imm, na.rm = TRUE),
            variance = var(ravlt_imm, na.rm = TRUE))
```


### Outliers using IQR method
Two outliers at the low end of the distribution have been identified.
```{r}
data %>% 
  dplyr::select(., subject, ravlt_imm) %>%
  identify_outliers("ravlt_imm")
  
```

### Variance is still larger than mean even after removing outliers
```{r}
data %>%
  filter(ravlt_imm > 13) %>%
  summarise(count = n(),
            mean = mean(ravlt_imm, na.rm = TRUE),
            variance = var(ravlt_imm, na.rm = TRUE))

data <- data %>%
  filter(ravlt_imm > 13)
```

### Break down of participant sex
A Chi-square test of proportion reveals that the number of females in the sample is not significantly different than the number of males.
```{r}
count(data, vars = sex)

# Chi Square test of proportion
prop.test(x = 125, n = 241, alternative = "two.sided")
```


```{r, plot and model function, echo = FALSE}
plot_and_model <- function(arg_1, arg_2, arg_3){
  colors = c("#440154FF","#1565c0")
  
  # Plot a histogram of graph metric values
  p1 <- ggplot(arg_1, aes_string(x = arg_2, color = "sex", fill = "sex")) +
  geom_histogram(alpha = 0.7) +
  scale_color_manual(values = colors) +
  scale_fill_manual(values = colors) +
  facet_grid(col = vars(sex))
  #print(p1)

  # Plot a scatter plot of the graph metric by cognitive measure
  p2 <- ggplot(arg_1, aes_string(x = arg_2, y = arg_3, color = "sex")) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = colors)
  #print(p2)
  
  
  # model
  f <- as.formula(paste(arg_3, arg_2, sep = "~"))
  model <- glm.nb(f, data = arg_1)
  print(summary(model))
  
  # Confidence Intervals
  cat("\n")
  print("Confidence Intervals:")
  print((est <- cbind(Estimate = coef(model), confint(model))))

  # Incident Ratios
  cat("\n")
  print("Incident Ratios:")
  print(exp(est))
  
  print(p1)
  print(p2)
  
  
# fits <- fitted(model)
# resids <- resid(model)
# plot(x = fits,
#      y = resids,
#      xlab = "fitted values",
#      ylab = "residuals")
# abline(h = 0, lty = 2)
  
}
```


### Modeling: Modularity
```{r}
# Modularity
plot_and_model(data, "mod", "ravlt_imm")
```

### Modeling: Clustering Coefficient
```{r}
# Clustering
plot_and_model(data, "clust", "ravlt_imm")
```


### Modeling: Characteristic Path Length
```{r}
# Characteristic Path Length
plot_and_model(data, "cpath", "ravlt_imm")
```


### Modeling: Global Efficiency
```{r}
# Global Efficiency
plot_and_model(data, "geff", "ravlt_imm")
```



<!-- ### RAVLT Learning -->
<!-- Learning is calculated as the score on Trial 5 minus Trial 1 and measures learning during the first 5 trials. -->
<!-- ```{r, warning=FALSE, message=FALSE} -->
<!-- ggplot(data, aes(x = ravlt_lrn)) + -->
<!--   geom_histogram(color = "#1565c0", fill = "#1565c0", alpha = 0.7) -->
<!-- ``` -->

<!-- ### RAVLT Forgetting Short Delay -->
<!-- Forgetting short delay is calculated as the score of Trial 5 minus the score on Trial 6. -->
<!-- ```{r, warning=FALSE, message=FALSE} -->
<!-- ggplot(data, aes(x = ravlt_fgt_sd)) + -->
<!--   geom_histogram(color = "#1565c0", fill = "#1565c0", alpha = 0.7) -->
<!-- ``` -->


<!-- ### RAVLT Percent Forgetting Short Delay -->
<!-- Percent forgetting is calculated as the forgetting short delay score divided by Trial 5. -->
<!-- ```{r} -->
<!-- ggplot(data, aes(x = ravlt_pcnt_fgt_sd)) + -->
<!--   geom_histogram(color = "#1565c0", fill = "#1565c0", alpha = 0.7) -->
<!-- ``` -->

<!-- ### RAVLT Percent Forgetting Long Delay -->
<!-- Forgetting long delay is calculated as the score of Trial 5 minus the score on Trial 7. -->

<!-- ```{r} -->
<!-- ggplot(data, aes(x = ravlt_pcnt_fgt_ld)) + -->
<!--   geom_histogram(color = "#1565c0", fill = "#1565c0", alpha = 0.7) -->
<!-- ``` -->

<!-- ### RAVLT Forgetting Long Delay -->
<!-- Percent forgetting long delay is calculated as the forgetting long delay score divided by Trial 7. -->

<!-- ```{r} -->
<!-- ggplot(data, aes(x = ravlt_fgt_ld)) + -->
<!--   geom_histogram(color = "#1565c0", fill = "#1565c0", alpha = 0.7) -->
<!-- ``` -->


